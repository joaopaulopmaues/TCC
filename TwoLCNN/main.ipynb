{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Triage API Key 9d13b781fa843f8811e8a4c0776750b1e351937b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#searching for sample-id using sha256\n",
    "\n",
    "triage search tag:search-string\n",
    "\n",
    "ex: triage search sha256:88ee23d0001b325653602351eb898af0ab82a7f8c2413d1f44fea7557c46eabb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparing the csvs into binary matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import numpy as np\n",
    "from encoding_dataset import unite_csvs_into_csv, encoding_processed_json\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Uniting all csvs from XRan dataset in one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CSVs from XRan's dataset\n",
    "L=[\"/Users/joaopaulopmaues/Downloads/tensorflow-test/TCC/XRan/XRan-main/ransomware/VirusShare.csv\",\\\n",
    "    \"/Users/joaopaulopmaues/Downloads/tensorflow-test/TCC/XRan/XRan-main/ransomware/ISOT.csv\",\\\n",
    "    \"/Users/joaopaulopmaues/Downloads/tensorflow-test/TCC/XRan/XRan-main/ransomware/Sorel20M.csv\",\\\n",
    "   \"/Users/joaopaulopmaues/Downloads/tensorflow-test/TCC/XRan/XRan-main/benign/Benign.csv\"]\n",
    "\n",
    "WholeDatasetOutputCsv=\"/Users/joaopaulopmaues/Downloads/tensorflow-test/TCC/XRan/XRan-main/wholeDataset-2.csv\"\n",
    "\n",
    "unite_csvs_into_csv(L,WholeDatasetOutputCsv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Turning the datasets into csvs with binary matrices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Those matrices have 14 columns for each 14 digit binary encoded word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/joaopaulopmaues/Downloads/tensorflow-test/TCC/XRan/TwoLCNN/encoding_dataset.py:81: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  df_copy.iloc[:, n1:n2] = df_copy.iloc[:, n1:n2].applymap(lambda x: word_to_binary.get(x, x))\n",
      "/Users/joaopaulopmaues/Downloads/tensorflow-test/TCC/XRan/TwoLCNN/encoding_dataset.py:81: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  df_copy.iloc[:, n1:n2] = df_copy.iloc[:, n1:n2].applymap(lambda x: word_to_binary.get(x, x))\n",
      "/Users/joaopaulopmaues/Downloads/tensorflow-test/TCC/XRan/TwoLCNN/encoding_dataset.py:81: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  df_copy.iloc[:, n1:n2] = df_copy.iloc[:, n1:n2].applymap(lambda x: word_to_binary.get(x, x))\n"
     ]
    }
   ],
   "source": [
    "encoding_processed_json([\"testingmain_14/Malign/VirusShare\",\"testingmain_14/Malign/ISOT\",\"testingmain_14/Malign/Sorel\",\"testingmain_14/Benign\"],WholeDatasetOutputCsv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Other tryings with the XRan dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Montando o dataset em csvs de 528x12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nimport pandas as pd\\n\\n# Supondo que \\'dfx\\' seja o DataFrame final gerado anteriormente\\n# dfx deve ter 12 colunas e múltiplas linhas\\n# Aqui está um exemplo de criação de \\'dfx\\' para fins ilustrativos\\n# Remova ou substitua esta parte pelo seu DataFrame real\\n# ---------------------------------------------------------\\n# Exemplo:\\n# rows = 1040  # Por exemplo, 2 amostras de 520 cada\\n# cols = 12\\n# dfx = pd.DataFrame([[i % 2 for _ in range(cols)] for i in range(rows)])\\n# ---------------------------------------------------------\\n\\n# Configurações\\nchunk_size = 520  # Número de linhas por pedaço\\ninsert_positions = [500, 510]  # Posições para inserção (1-based)\\ninsert_row = [-1] * 12  # Linha a ser inserida\\nadditional_end_rows = 6  # Número de linhas \\'-1\\' a serem adicionadas ao final\\n\\n# Função para processar cada pedaço\\ndef process_chunk(chunk):\\n    # Converter o pedaço para list para facilitar inserções\\n    chunk_list = chunk.values.tolist()\\n    \\n    # Inserir a primeira linha de \\'-1\\'s após a linha 500 (índice 500)\\n    # Ajuste para 0-based index\\n    insert_idx_1 = 500\\n    if len(chunk_list) > insert_idx_1:\\n        chunk_list.insert(insert_idx_1, insert_row)\\n    else:\\n        # Se o pedaço tiver menos de 500 linhas, insira ao final\\n        chunk_list.append(insert_row)\\n    \\n    # Inserir a segunda linha de \\'-1\\'s após 10 linhas da primeira inserção\\n    insert_idx_2 = insert_idx_1 + 10 + 1  # +1 devido à primeira inserção\\n    if len(chunk_list) > insert_idx_2:\\n        chunk_list.insert(insert_idx_2, insert_row)\\n    else:\\n        # Se não houver linhas suficientes, insira ao final\\n        chunk_list.append(insert_row)\\n    \\n    # Adicionar 6 linhas de \\'-1\\'s ao final\\n    for _ in range(additional_end_rows):\\n        chunk_list.append(insert_row)\\n    \\n    # Converter de volta para DataFrame\\n    processed_chunk = pd.DataFrame(chunk_list)\\n    \\n    return processed_chunk\\n\\n# Dividir \\'dfx\\' em pedaços de 520 linhas\\nnum_chunks = len(dfx) // chunk_size\\nremainder = len(dfx) % chunk_size\\nif remainder != 0:\\n    num_chunks += 1  # Adicionar um pedaço adicional para as linhas restantes\\n\\nfor i in range(num_chunks):\\n    start = i * chunk_size\\n    end = start + chunk_size\\n    chunk = dfx.iloc[start:end].copy()\\n    \\n    # Processar o pedaço\\n    processed_chunk = process_chunk(chunk)\\n    \\n    # Verificar se o pedaço tem 520 linhas antes das inserções\\n    # Isso garante que, após inserções, teremos 528 linhas\\n    if len(chunk) == chunk_size:\\n        expected_rows = chunk_size + 2 + additional_end_rows  # 520 + 2 inserções + 6 finais\\n    else:\\n        # Para o último pedaço que pode ter menos de 520 linhas\\n        expected_rows = len(chunk) + 2 + additional_end_rows\\n    actual_rows = len(processed_chunk)\\n    \\n    # Opcional: Verificar se o número de linhas está correto\\n    assert actual_rows == expected_rows, f\"Pedaço {i+1} tem {actual_rows} linhas, esperado {expected_rows}.\"\\n    \\n    # Salvar o pedaço processado em um arquivo CSV\\n    filename = f\"VirusShare_528_12_Sample_{i + 1}.csv\"\\n    processed_chunk.to_csv(filename, index=False, header=False)\\n    #print(f\"Arquivo salvo: {filename} com {actual_rows} linhas.\")\\n'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "import pandas as pd\n",
    "\n",
    "# Supondo que 'dfx' seja o DataFrame final gerado anteriormente\n",
    "# dfx deve ter 12 colunas e múltiplas linhas\n",
    "# Aqui está um exemplo de criação de 'dfx' para fins ilustrativos\n",
    "# Remova ou substitua esta parte pelo seu DataFrame real\n",
    "# ---------------------------------------------------------\n",
    "# Exemplo:\n",
    "# rows = 1040  # Por exemplo, 2 amostras de 520 cada\n",
    "# cols = 12\n",
    "# dfx = pd.DataFrame([[i % 2 for _ in range(cols)] for i in range(rows)])\n",
    "# ---------------------------------------------------------\n",
    "\n",
    "# Configurações\n",
    "chunk_size = 520  # Número de linhas por pedaço\n",
    "insert_positions = [500, 510]  # Posições para inserção (1-based)\n",
    "insert_row = [-1] * 12  # Linha a ser inserida\n",
    "additional_end_rows = 6  # Número de linhas '-1' a serem adicionadas ao final\n",
    "\n",
    "# Função para processar cada pedaço\n",
    "def process_chunk(chunk):\n",
    "    # Converter o pedaço para list para facilitar inserções\n",
    "    chunk_list = chunk.values.tolist()\n",
    "    \n",
    "    # Inserir a primeira linha de '-1's após a linha 500 (índice 500)\n",
    "    # Ajuste para 0-based index\n",
    "    insert_idx_1 = 500\n",
    "    if len(chunk_list) > insert_idx_1:\n",
    "        chunk_list.insert(insert_idx_1, insert_row)\n",
    "    else:\n",
    "        # Se o pedaço tiver menos de 500 linhas, insira ao final\n",
    "        chunk_list.append(insert_row)\n",
    "    \n",
    "    # Inserir a segunda linha de '-1's após 10 linhas da primeira inserção\n",
    "    insert_idx_2 = insert_idx_1 + 10 + 1  # +1 devido à primeira inserção\n",
    "    if len(chunk_list) > insert_idx_2:\n",
    "        chunk_list.insert(insert_idx_2, insert_row)\n",
    "    else:\n",
    "        # Se não houver linhas suficientes, insira ao final\n",
    "        chunk_list.append(insert_row)\n",
    "    \n",
    "    # Adicionar 6 linhas de '-1's ao final\n",
    "    for _ in range(additional_end_rows):\n",
    "        chunk_list.append(insert_row)\n",
    "    \n",
    "    # Converter de volta para DataFrame\n",
    "    processed_chunk = pd.DataFrame(chunk_list)\n",
    "    \n",
    "    return processed_chunk\n",
    "\n",
    "# Dividir 'dfx' em pedaços de 520 linhas\n",
    "num_chunks = len(dfx) // chunk_size\n",
    "remainder = len(dfx) % chunk_size\n",
    "if remainder != 0:\n",
    "    num_chunks += 1  # Adicionar um pedaço adicional para as linhas restantes\n",
    "\n",
    "for i in range(num_chunks):\n",
    "    start = i * chunk_size\n",
    "    end = start + chunk_size\n",
    "    chunk = dfx.iloc[start:end].copy()\n",
    "    \n",
    "    # Processar o pedaço\n",
    "    processed_chunk = process_chunk(chunk)\n",
    "    \n",
    "    # Verificar se o pedaço tem 520 linhas antes das inserções\n",
    "    # Isso garante que, após inserções, teremos 528 linhas\n",
    "    if len(chunk) == chunk_size:\n",
    "        expected_rows = chunk_size + 2 + additional_end_rows  # 520 + 2 inserções + 6 finais\n",
    "    else:\n",
    "        # Para o último pedaço que pode ter menos de 520 linhas\n",
    "        expected_rows = len(chunk) + 2 + additional_end_rows\n",
    "    actual_rows = len(processed_chunk)\n",
    "    \n",
    "    # Opcional: Verificar se o número de linhas está correto\n",
    "    assert actual_rows == expected_rows, f\"Pedaço {i+1} tem {actual_rows} linhas, esperado {expected_rows}.\"\n",
    "    \n",
    "    # Salvar o pedaço processado em um arquivo CSV\n",
    "    filename = f\"VirusShare_528_12_Sample_{i + 1}.csv\"\n",
    "    processed_chunk.to_csv(filename, index=False, header=False)\n",
    "    #print(f\"Arquivo salvo: {filename} com {actual_rows} linhas.\")\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Montando o dataset com csvs de 84x84"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sem alternancia entre chamadas api, dlls criadas e mutex abertos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ndef gen_csvs_84_no_altern_for_one_dataset(filename,dfx,nsample=1):\\n    # Configurações\\n    new_cols = 84  # Número de colunas no novo DataFrame\\n    group_size = new_cols//dfx.columns.stop  # Número de linhas originais por nova linha #Era 7 quando somente VirusShare no dataset\\n    padding_value = -1  # Valor para preencher espaços vazios\\n    separator_row = [-1] * new_cols  # Linha separadora\\n    padding_rows = 4  # Linhas adicionais no início e no final do arquivo #Era 3 quando somente VirusShare no dataset\\n    rows_per_sample=520\\n\\n    # Função para transformar um grupo de linhas do DataFrame\\n    def transform_to_wide_format(df, group_size, new_cols, padding_value):\\n        wide_data = []\\n        for i in range(0, len(df), group_size):\\n            group = df.iloc[i:i + group_size].values.flatten()  # Unir linhas do grupo\\n            if len(group) < new_cols:\\n                # Preencher com -1 se o grupo for menor que o necessário\\n                group = np.pad(group, (0, new_cols - len(group)), constant_values=padding_value)\\n            wide_data.append(group[:new_cols])  # Garantir exatamente `new_cols` colunas\\n        return pd.DataFrame(wide_data)\\n\\n    # Processar uma amostra completa de 520 linhas\\n    def process_sample(df, new_cols, group_size, padding_value, separator_row, padding_rows):\\n        # Transformar as primeiras 500 linhas\\n        first_block = transform_to_wide_format(df.iloc[:500], group_size, new_cols, padding_value)\\n        \\n        # Processar os dois blocos de 10 linhas cada\\n        second_block = transform_to_wide_format(df.iloc[500:510], group_size, new_cols, padding_value)\\n        third_block = transform_to_wide_format(df.iloc[510:], group_size, new_cols, padding_value)\\n        \\n        # Montar o DataFrame final com separadores\\n        combined = pd.concat(\\n            [\\n                pd.DataFrame([separator_row] * padding_rows),  # Linhas iniciais de -1\\n                first_block,  # Primeiro bloco (500 linhas)\\n                pd.DataFrame([separator_row]),  # Separador\\n                second_block,  # Segundo bloco (10 linhas)\\n                pd.DataFrame([separator_row]),  # Separador\\n                third_block,  # Terceiro bloco (10 linhas)\\n                pd.DataFrame([separator_row] * padding_rows),  # Linhas finais de -1\\n            ],\\n            ignore_index=True,\\n        )\\n        return combined\\n\\n    # Dividir o DataFrame em amostras de 520 linhas e processar cada uma\\n    for sample_idx in range(0, len(dfx), rows_per_sample):\\n        sample = dfx.iloc[sample_idx:sample_idx + rows_per_sample]  # Extrair a amostra atual\\n        \\n        # Processar a amostra para o formato 84x84\\n        processed_sample = process_sample(sample, new_cols, group_size, padding_value, separator_row, padding_rows)\\n        \\n        # Verificar se o DataFrame resultante tem exatamente 84x84\\n        assert processed_sample.shape == (98, 84), f\"Erro no tamanho da amostra {sample_idx // rows_per_sample + 1}\"\\n        \\n        # Salvar o grupo processado em um arquivo CSV\\n        filenamer = f\"{filename}_84/Sample_{nsample}.csv\"\\n        processed_sample.to_csv(filenamer, index=False, header=False)\\n        nsample+=1\\n    #print(f\"Arquivo salvo: {filename} com tamanho {processed_group.shape}\")\\n   \\n\\ni=0\\nwhile(i<4):\\n    begin=idx[i][0]\\n    end=idx[i][1]\\n    dfx_aux=expanded_columns.iloc[begin:end].copy()\\n    gen_csvs_84_no_altern_for_one_dataset(filenames[i],dfx_aux)\\n    i+=1\\n'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#definirei uma função para realizar a criação de csvs para um mesmo dataset\n",
    "#datasets existentes: benign, virusshare, Sorel20M e ISOT.\n",
    "'''\n",
    "def gen_csvs_84_no_altern_for_one_dataset(filename,dfx,nsample=1):\n",
    "    # Configurações\n",
    "    new_cols = 84  # Número de colunas no novo DataFrame\n",
    "    group_size = new_cols//dfx.columns.stop  # Número de linhas originais por nova linha #Era 7 quando somente VirusShare no dataset\n",
    "    padding_value = -1  # Valor para preencher espaços vazios\n",
    "    separator_row = [-1] * new_cols  # Linha separadora\n",
    "    padding_rows = 4  # Linhas adicionais no início e no final do arquivo #Era 3 quando somente VirusShare no dataset\n",
    "    rows_per_sample=520\n",
    "\n",
    "    # Função para transformar um grupo de linhas do DataFrame\n",
    "    def transform_to_wide_format(df, group_size, new_cols, padding_value):\n",
    "        wide_data = []\n",
    "        for i in range(0, len(df), group_size):\n",
    "            group = df.iloc[i:i + group_size].values.flatten()  # Unir linhas do grupo\n",
    "            if len(group) < new_cols:\n",
    "                # Preencher com -1 se o grupo for menor que o necessário\n",
    "                group = np.pad(group, (0, new_cols - len(group)), constant_values=padding_value)\n",
    "            wide_data.append(group[:new_cols])  # Garantir exatamente `new_cols` colunas\n",
    "        return pd.DataFrame(wide_data)\n",
    "\n",
    "    # Processar uma amostra completa de 520 linhas\n",
    "    def process_sample(df, new_cols, group_size, padding_value, separator_row, padding_rows):\n",
    "        # Transformar as primeiras 500 linhas\n",
    "        first_block = transform_to_wide_format(df.iloc[:500], group_size, new_cols, padding_value)\n",
    "        \n",
    "        # Processar os dois blocos de 10 linhas cada\n",
    "        second_block = transform_to_wide_format(df.iloc[500:510], group_size, new_cols, padding_value)\n",
    "        third_block = transform_to_wide_format(df.iloc[510:], group_size, new_cols, padding_value)\n",
    "        \n",
    "        # Montar o DataFrame final com separadores\n",
    "        combined = pd.concat(\n",
    "            [\n",
    "                pd.DataFrame([separator_row] * padding_rows),  # Linhas iniciais de -1\n",
    "                first_block,  # Primeiro bloco (500 linhas)\n",
    "                pd.DataFrame([separator_row]),  # Separador\n",
    "                second_block,  # Segundo bloco (10 linhas)\n",
    "                pd.DataFrame([separator_row]),  # Separador\n",
    "                third_block,  # Terceiro bloco (10 linhas)\n",
    "                pd.DataFrame([separator_row] * padding_rows),  # Linhas finais de -1\n",
    "            ],\n",
    "            ignore_index=True,\n",
    "        )\n",
    "        return combined\n",
    "\n",
    "    # Dividir o DataFrame em amostras de 520 linhas e processar cada uma\n",
    "    for sample_idx in range(0, len(dfx), rows_per_sample):\n",
    "        sample = dfx.iloc[sample_idx:sample_idx + rows_per_sample]  # Extrair a amostra atual\n",
    "        \n",
    "        # Processar a amostra para o formato 84x84\n",
    "        processed_sample = process_sample(sample, new_cols, group_size, padding_value, separator_row, padding_rows)\n",
    "        \n",
    "        # Verificar se o DataFrame resultante tem exatamente 84x84\n",
    "        assert processed_sample.shape == (98, 84), f\"Erro no tamanho da amostra {sample_idx // rows_per_sample + 1}\"\n",
    "        \n",
    "        # Salvar o grupo processado em um arquivo CSV\n",
    "        filenamer = f\"{filename}_84/Sample_{nsample}.csv\"\n",
    "        processed_sample.to_csv(filenamer, index=False, header=False)\n",
    "        nsample+=1\n",
    "    #print(f\"Arquivo salvo: {filename} com tamanho {processed_group.shape}\")\n",
    "   \n",
    "\n",
    "i=0\n",
    "while(i<4):\n",
    "    begin=idx[i][0]\n",
    "    end=idx[i][1]\n",
    "    dfx_aux=expanded_columns.iloc[begin:end].copy()\n",
    "    gen_csvs_84_no_altern_for_one_dataset(filenames[i],dfx_aux)\n",
    "    i+=1\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Com alternância entre linhas de chamadas api, dlls criadas e mutex abertos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# Configurações\\nnew_cols = 84\\ngroup_size = 6\\npadding_value = -1\\nseparator_row = [-1] * new_cols\\nrows_per_sample=520\\n\\ndef transform_section_to_wide(input_df, new_cols, padding_value):\\n    flattened = input_df.values.flatten()  # Achatar as 10 linhas de 12 colunas\\n    result = []\\n\\n    # Dividir em linhas de 7 elementos (como sugerido)\\n    for i in range(0, len(flattened), group_size):\\n        line = flattened[i:i + group_size]\\n        if len(line) < group_size:\\n            line = np.pad(line, (0, group_size - len(line)), constant_values=padding_value)\\n        result.append(line)\\n\\n    # Agora, a última linha precisa de padding até completar 84 colunas\\n    if len(result) == 1:\\n        # Preencher com -1 até completar 84\\n        result[0] = np.pad(result[0], (0, new_cols - len(result[0])), constant_values=padding_value)\\n    return result\\n\\n# Função para transformar múltiplas linhas em linhas \"wide\"\\ndef transform_to_wide_format(input_df, group_size, new_cols, padding_value):\\n    wide_data = []\\n    for i in range(0, len(input_df), group_size):\\n        group = input_df.iloc[i:i + group_size].values.flatten()\\n        if len(group) < new_cols:\\n            group = np.pad(group, (0, new_cols - len(group)), constant_values=padding_value)\\n        wide_data.append(group[:new_cols])\\n    return wide_data\\n\\n# Função para processar uma amostra em estrutura 84x84\\ndef process_sample(df, new_cols, group_size, padding_value, separator_row):\\n    result = []\\n\\n    # Seção |A|: 12 linhas\\n    result.extend(transform_to_wide_format(df.iloc[:84], group_size, new_cols, padding_value))\\n    result.append(separator_row)\\n\\n    # Seção |D|: 10 linhas divididas em 2 grupos de 7 elementos\\n    result.extend(transform_to_wide_format(df.iloc[500:507], group_size, new_cols, padding_value))\\n    result.append(separator_row)\\n\\n    # Seção |A|: 6 linhas\\n    result.extend(transform_to_wide_format(df.iloc[84:126], group_size, new_cols, padding_value))\\n    result.append(separator_row)\\n\\n    # Seção |M|: 10 linhas divididas em 2 grupos de 7 elementos\\n    result.extend(transform_to_wide_format(df.iloc[510:517], group_size, new_cols, padding_value))\\n    result.append(separator_row)\\n\\n    # Seção |A|: 36 linhas\\n    result.extend(transform_to_wide_format(df.iloc[126:378], group_size, new_cols, padding_value))\\n    result.append(separator_row)\\n\\n    # Seção |D|: 10 linhas divididas em 2 grupos de 7 elementos\\n    result.extend(transform_to_wide_format(df.iloc[507:510], group_size, new_cols, padding_value))\\n    result.append(separator_row)\\n\\n    # Seção |A|: 6 linhas\\n    result.extend(transform_to_wide_format(df.iloc[378:420], group_size, new_cols, padding_value))\\n    result.append(separator_row)\\n\\n    # Seção |M|: 10 linhas divididas em 2 grupos de 7 elementos\\n    result.extend(transform_to_wide_format(df.iloc[517:520], group_size, new_cols, padding_value))\\n    result.append(separator_row)\\n\\n    # Seção |A|: 12 linhas\\n    result.extend(transform_to_wide_format(df.iloc[420:504], group_size, new_cols, padding_value))\\n    \\n    # Garantir exatamente 84 linhas no total\\n    assert len(result) == 84, f\"Erro no tamanho final da amostra: {len(result)} linhas geradas\"\\n    return pd.DataFrame(result)\\n\\n# Processar cada amostra do DataFrame original\\nfor sample_idx in range(0, len(dfx), rows_per_sample):\\n    sample = dfx.iloc[sample_idx:sample_idx + rows_per_sample]  # Extrair a amostra atual\\n    \\n    # Processar a amostra para o formato 84x84\\n    processed_sample = process_sample(sample, new_cols, group_size, padding_value, separator_row)\\n    \\n    # Salvar o resultado como arquivo CSV\\n    filename = f\"VirusShare_84_Alternado/Sample_{sample_idx // rows_per_sample + 1}.csv\"\\n    processed_sample.to_csv(filename, index=False, header=False)\\n    #print(f\"Arquivo salvo: {filename} com tamanho {processed_sample.shape}\")\\n'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "# Configurações\n",
    "new_cols = 84\n",
    "group_size = 6\n",
    "padding_value = -1\n",
    "separator_row = [-1] * new_cols\n",
    "rows_per_sample=520\n",
    "\n",
    "def transform_section_to_wide(input_df, new_cols, padding_value):\n",
    "    flattened = input_df.values.flatten()  # Achatar as 10 linhas de 12 colunas\n",
    "    result = []\n",
    "\n",
    "    # Dividir em linhas de 7 elementos (como sugerido)\n",
    "    for i in range(0, len(flattened), group_size):\n",
    "        line = flattened[i:i + group_size]\n",
    "        if len(line) < group_size:\n",
    "            line = np.pad(line, (0, group_size - len(line)), constant_values=padding_value)\n",
    "        result.append(line)\n",
    "\n",
    "    # Agora, a última linha precisa de padding até completar 84 colunas\n",
    "    if len(result) == 1:\n",
    "        # Preencher com -1 até completar 84\n",
    "        result[0] = np.pad(result[0], (0, new_cols - len(result[0])), constant_values=padding_value)\n",
    "    return result\n",
    "\n",
    "# Função para transformar múltiplas linhas em linhas \"wide\"\n",
    "def transform_to_wide_format(input_df, group_size, new_cols, padding_value):\n",
    "    wide_data = []\n",
    "    for i in range(0, len(input_df), group_size):\n",
    "        group = input_df.iloc[i:i + group_size].values.flatten()\n",
    "        if len(group) < new_cols:\n",
    "            group = np.pad(group, (0, new_cols - len(group)), constant_values=padding_value)\n",
    "        wide_data.append(group[:new_cols])\n",
    "    return wide_data\n",
    "\n",
    "# Função para processar uma amostra em estrutura 84x84\n",
    "def process_sample(df, new_cols, group_size, padding_value, separator_row):\n",
    "    result = []\n",
    "\n",
    "    # Seção |A|: 12 linhas\n",
    "    result.extend(transform_to_wide_format(df.iloc[:84], group_size, new_cols, padding_value))\n",
    "    result.append(separator_row)\n",
    "\n",
    "    # Seção |D|: 10 linhas divididas em 2 grupos de 7 elementos\n",
    "    result.extend(transform_to_wide_format(df.iloc[500:507], group_size, new_cols, padding_value))\n",
    "    result.append(separator_row)\n",
    "\n",
    "    # Seção |A|: 6 linhas\n",
    "    result.extend(transform_to_wide_format(df.iloc[84:126], group_size, new_cols, padding_value))\n",
    "    result.append(separator_row)\n",
    "\n",
    "    # Seção |M|: 10 linhas divididas em 2 grupos de 7 elementos\n",
    "    result.extend(transform_to_wide_format(df.iloc[510:517], group_size, new_cols, padding_value))\n",
    "    result.append(separator_row)\n",
    "\n",
    "    # Seção |A|: 36 linhas\n",
    "    result.extend(transform_to_wide_format(df.iloc[126:378], group_size, new_cols, padding_value))\n",
    "    result.append(separator_row)\n",
    "\n",
    "    # Seção |D|: 10 linhas divididas em 2 grupos de 7 elementos\n",
    "    result.extend(transform_to_wide_format(df.iloc[507:510], group_size, new_cols, padding_value))\n",
    "    result.append(separator_row)\n",
    "\n",
    "    # Seção |A|: 6 linhas\n",
    "    result.extend(transform_to_wide_format(df.iloc[378:420], group_size, new_cols, padding_value))\n",
    "    result.append(separator_row)\n",
    "\n",
    "    # Seção |M|: 10 linhas divididas em 2 grupos de 7 elementos\n",
    "    result.extend(transform_to_wide_format(df.iloc[517:520], group_size, new_cols, padding_value))\n",
    "    result.append(separator_row)\n",
    "\n",
    "    # Seção |A|: 12 linhas\n",
    "    result.extend(transform_to_wide_format(df.iloc[420:504], group_size, new_cols, padding_value))\n",
    "    \n",
    "    # Garantir exatamente 84 linhas no total\n",
    "    assert len(result) == 84, f\"Erro no tamanho final da amostra: {len(result)} linhas geradas\"\n",
    "    return pd.DataFrame(result)\n",
    "\n",
    "# Processar cada amostra do DataFrame original\n",
    "for sample_idx in range(0, len(dfx), rows_per_sample):\n",
    "    sample = dfx.iloc[sample_idx:sample_idx + rows_per_sample]  # Extrair a amostra atual\n",
    "    \n",
    "    # Processar a amostra para o formato 84x84\n",
    "    processed_sample = process_sample(sample, new_cols, group_size, padding_value, separator_row)\n",
    "    \n",
    "    # Salvar o resultado como arquivo CSV\n",
    "    filename = f\"VirusShare_84_Alternado/Sample_{sample_idx // rows_per_sample + 1}.csv\"\n",
    "    processed_sample.to_csv(filename, index=False, header=False)\n",
    "    #print(f\"Arquivo salvo: {filename} com tamanho {processed_sample.shape}\")\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
